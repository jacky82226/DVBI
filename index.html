<!DOCTYPE html>
<html lang="zh-Hant">
	<head>
		<meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Drone-View Building Idetntification</title>
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap-theme.min.css">
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js">
		<!--<script src="http://code.jquery.com/jquery-1.9.1.js"></script>-->
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
		<script src="magic.js"></script>
		<link rel="stylesheet" href="styles.css">
	</head>

	<body>
		<div id="header">
		<h2>Drone-View Building Identification by Cross-View Visual Learning and Relative Spatial Estimation</h2>
		</div>
		<div id="container">
			<div id ="main">
			<img width=540 src="first.png" />
			<ul>
				<li>Authors</li>
				<a href="http://www.cmlab.csie.ntu.edu.tw/~jacky82226/">Chun-Wei Chen</a>, <a href ="http://www.cmlab.csie.ntu.edu.tw/~kuonini/">Yin-Hsi Kuo</a>, <a href="https://weitang114.github.io">Tang Lee</a>, <a href="http://www.cmlab.csie.ntu.edu.tw/~steven413d/">Cheng-Han Lee</a>, <a href="http://winstonhsu.info/">Winston Hsu</a>
		        <li>Abstract</li>
		        <p class="content">
Drones become popular recently and equip more sensors than traditional cameras, which bring emerging applications and research. To enable drone-based applications, providing related information (e.g., building) to understand the environment around the drone is essential. We frame this drone-view building identification as building retrieval problem: given a building (multimodal query) with its images, geolocation and drone's current location, we aim to retrieve the most likely proposal (building candidate) on a drone-view image. Despite few annotated drone-view images to date, there are many images of other views from the Web, like ground-level, street-view and aerial images. Thus, we propose a cross-view triplet neural network to learn visual similarity between drone-view and other views, further design relative spatial estimation of each proposal and the drone, and collect new drone-view datasets for the task. Our method outperforms triplet neural network by 0.12 mAP. (i.e., 22.9 to 35.0, +53% in a sub-dataset [LA])
				</p>
				<li>Publication</li>
				Chun-Wei Chen, Yin-Hsi Kuo, Tang Lee, Chen-Han Lee, Winston Hsu. Drone-View Building Identification by Cross-View Visual Learning and Relative Spatial Estimation, CVPR 2018 Workshop, VOCVALC 2018, 2nd International workshop on Visual Odometry and Computer Vision Applications based Location Clues
				<li>Codes</li>
				<a href="https://github.com/jacky82226/drone_view_building_identification">[github]</a>
		        <li>Dataset</li>
		        <div style="color: red">Please notice that this dataset is made available for academic research purpose only. Some images are collected from Google and Instagram, and the copyright belongs to the original owners. If any of the images belongs to you and you would like it removed, please kindly <a href="mailto: jacky82226@gmail.com">inform us</a>, we will remove it from our dataset immediately.</div>
		        	<ol>
		        		<li>Drone-BR</li>
		        		<img width=900 src="supplemental/Drone-BR.png"/>
		        		<p>
							We collect drone-view images with our drone (DJI 3) by designing and using an app for automatically recording all drone sensors including GPS tracker, compass and altimeter while capturing images. We fix the angle of depression as 20&deg;, raise the drone to 40m to 90m height and capture images with multiple buildings. Each image consists of 1-12 buildings as queries which have corresponding bounding boxes on drone-view images. We collect 80 images and annotate 108 buildings with bounding boxes on them at 3 locations under different weather. Besides, with Google Places API, we gather other information for these buildings including name, latitude and longitude. 

							We gather ground-level, street-view and aerial images for each building. Since the same buildings may appear in different images, there are 585 building queries in total, which is <b>Drone-BR</b>. Since buildings may be occluded or in low resolution, it requires great effort to annotate them with their corresponding high-resolution drone-view images (mostly 3840x2160). In our setting, we search each building query in 200 proposals on a drone-view image, which means the Drone-BR can be also described as 585 queries with 16,000 (200*80) sub-images for searching. Our experiment results show that our method can perform better with limited training data and it is scalable because the building identification for each drone-view image is performed independently.		

		        		</p>
		        		<ul class="data">
		        			<li>Building retrieval/identification task</li>
		        			<li>80 drone-view images: 18 [1920x1080] + 62 [3840x2160] (we collected)</li>
		        			<li>585 queries (16,000 proposals)</li>
		        			<li>(a) Building queries from web (b) Matched bounding boxes (c) Droneâ€™s geolocation for images</li>
		        			<li style="color: red">Because of the Google policy, we cannot provide the images colltected from Google directly. (i.e., ground-level images from <a href="https://images.google.com/">Google Images Search</a>, stree-view images from <a href="https://developers.google.com/streetview/">Google Stree-View</a> and aerial images from <a href="https://developers.google.com/maps/">Google Maps</a>) Nevertheless, these images can be obtained easily by automatically parsing codes with the buildings' names and GPS.</li>
		        			<li><a href="https://www.dropbox.com/sh/rb5ruf5r93gozgo/AAB1zbPwb1vXIBEODFNdaS0_a">[download]</a></li>
		        			<li>Drone-view images:</li>
		        			<ol>
		        				<li>frame/all/[filename].jpg: all of images</li>
		        				<li>frame/LocationA/test/[filename].jpg: images taken at Sun Yat-sen Memorial Hall</li>
		        				<li>frame/LocationB/test/[filename].jpg: images taken at Chiang Kai-shek Memorial Hall</li>
		        				<li>frame/LocationC/test/[filename].jpg: images taken at Daan Forest Park</li>
		        				<li>frame/WeatherA/test/[filename].jpg: images taken under the same weather</li>
		        				<li>frame/WeatherB/test/[filename].jpg: images taken under the same weather</li>
		        			</ol>
		        			<li>location.txt: [filename] [latitude] [longitude] [compass] [height] P.S.: compass: NESW(0, 90, 180, -90)</li>
		        			<li>Building proposals by Faster R-CNN for each image: faster_bb/[filename].txt</li>
		        			<li>Building queries for each image by <a href="https://developers.google.com/places/">Google Places API</a>: poi/[filename].txt
		        				<ul><li>
		        				[building name]\t[latitude]\t[longitude]\t[building type]\t[google image]\t[x1,y1,x2,y2]
		        				</li></ul>
		        			</li>


		        		</ul>
		   
		        		<li>Drone-BD</li>
		        		<p>
		        			On top of that, to the best of our knowledge, there are almost none drone-view images for building detection in any public datasets. In order to obtain building proposals with better quality, we also annotate 2,334 building bounding boxes on our 18 drone-view images and 185 images from Dronestagram website for training RPN [5]. This drone-view building detection dataset is called <b>Drone-BD</b>. 
		        		</p>
		        		<ul class="data">
		        			<li>Building detection task for proposals</li>
		        			<li>80 drone-view images: 18 [1920x1080] images (we collected) + 185 [1200x800] images (from Dronestagram)</li>
		        			<li>2,334 bounding boxes for buildings</li>
		        			<li><a href="https://www.dropbox.com/sh/hducbqmff17z9uj/AACJZUmViUWgu26Wj101Kimva">[download]</a></li>
		        			<li>
		        				Collected drone-view images: 
		        				<ol>
		        				<li>Collected/img/[filename].jpg</li>
		        				<li>Collected/annotation/[filename].txt: x1 y1 x2 y2 1</li>
		        				</ol>
		        			</li>
		        			<li>
		        				Dronestagram drone-view images:<ol>
		        				<li>Dronestagram/img/[filename].jpg</li>
		        				<li>Dronestagram/annotation/[filename].xml: tag([xmin], [ymin], [xmax], [ymax])
		        				</ol>
		        			</li>
		        		</ul>
		        		
		        		<li>IG-City8</li>
		        		<p>
		        			Due to the lack of drone-view data, we collect building images based on check-ins (locations) from Instagram and use the hashtag of #building and #buildings in 8 cities including New York, London, Paris, Hong Kong, Tokyo, Sydney, Berlin and San Francisco, to get building images. They are mostly ground-view or street-view images taken by users. We make buildings with the same location be a positive pair and with another random location be a negative pair to form a triplet. We manually remove some noisy images owing to not facing the correct building; eventually, there are 4,409 images, 848 buildings and 104,906 triplets in our Instagram building dataset called <b>IG-City8</b>. 
		        		</p>
		        		<ul class="data">
		        			<li>Building matching task in 8 cities</li>
		        			<li>4,409 [640x640] images (from Instagram) for 848 buildings</li>
		        			<li>104,906 triplets (209,812 pairs) for matched building images</li>
		        			<!--<li>(a) Matched building images (b) Date, check-ins, hashtags and geolocation for images</li>-->		
		        			<li><a href="https://www.dropbox.com/sh/gkojhos8uvv5hrf/AAD14P-qOAsqf_xVv4PC61Koa">[download]</a></li>
		        			<li>Images of the same building are in the same directory.</li>
		        		</ul>
		        		
		        	</ol>
		        <li>Experimental Results</li>
				<style type="text/css">
				.tg  {border-collapse:collapse;border-spacing:0;}
				.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
				.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
				.tg .tg-64g0{font-weight:bold;font-size:18px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:inherit;text-align:center;vertical-align:top}
				.tg .tg-k0up{font-size:18px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:inherit;text-align:center;vertical-align:top}
				.tg .tg-3dew{font-size:18px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;text-align:center;vertical-align:top}
				</style>
				<table class="tg" style="undefined;table-layout: fixed; width: 656px;">
				<colgroup>
				<col style="width: 193px">
				<col style="width: 119px">
				<col style="width: 167px">
				<col style="width: 177px">
				</colgroup>
				  <tr>
				    <th class="tg-k0up">Query images<br>mAP (%)</th>
				    <th class="tg-3dew">Aerial image</th>
				    <th class="tg-3dew">Street-view image<br></th>
				    <th class="tg-64g0">Ground-level image</th>
				  </tr>
				  <tr>
				    <td class="tg-k0up">ImageNet-AlexNet [1]</td>
				    <td class="tg-3dew">5.32</td>
				    <td class="tg-3dew">7.43</td>
				    <td class="tg-k0up">15.87</td>
				  </tr>
				  <tr>
				    <td class="tg-k0up">Places205-AlexNet [2]</td>
				    <td class="tg-3dew">6.98</td>
				    <td class="tg-3dew">9.27</td>
				    <td class="tg-k0up">20.55</td>
				  </tr>
				  <tr>
				    <td class="tg-64g0">IG-City8-Triplet [3]</td>
				    <td class="tg-3dew">7.38</td>
				    <td class="tg-3dew">13.06</td>
				    <td class="tg-64g0">21.61</td>
				  </tr>
				</table>
				<p>
					<b>Effect of initial models.</b> mAP (%) of various initial models on Drone-BR with 3 different types of 585 queries. It is critical to utilize related dataset (IG-City8) for getting better performance. Ground-level image is the most useful due to containing more visual information than other views.
				</p>
				<style type="text/css">
				.tg  {border-collapse:collapse;border-spacing:0;}
				.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
				.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
				.tg .tg-k0up{font-size:18px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:inherit;text-align:center;vertical-align:top}
				.tg .tg-3dew{font-size:18px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;text-align:center;vertical-align:top}
				</style>
				<table class="tg" style="undefined;table-layout: fixed; width: 675px;">
				<colgroup>
				<col style="width: 388px">
				<col style="width: 287px">
				</colgroup>
				  <tr>
				    <th class="tg-k0up">Methods</th>
				    <th class="tg-3dew">mAP (%) on 585 queries</th>
				  </tr>
				  <tr>
				    <td class="tg-k0up">Drone-angle (Da)</td>
				    <td class="tg-3dew">8.91</td>
				  </tr>
				  <tr>
				    <td class="tg-k0up">Drone-distance (Dx, Dy)</td>
				    <td class="tg-3dew">12.29</td>
				  </tr>
				  <tr>
				    <td class="tg-k0up">Drone-angle(Da) + Drone-distance (Dx, Dy)</td>
				    <td class="tg-3dew">16.23</td>
				  </tr>
				  <tr>
				    <td class="tg-3dew">IG-City8-Triplet [3] (Visual features)</td>
				    <td class="tg-3dew">21.61</td>
				  </tr>
				</table>
				<p>
					<b>Relative spatial estimation versus visual features.</b> The performance of relative spatial estimation is worse than visual features (even worse than methods without learning). Since there are multiple candidates with similar angle and distance, it is hard to distinguish them based on spatial estimation, which motivates us to leverage visual features.
				</p>

				<style type="text/css">
				.tg  {border-collapse:collapse;border-spacing:0;}
				.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:black;}
				.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:black;}
				.tg .tg-4saq{font-weight:bold;font-size:16px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:inherit;text-align:center;vertical-align:top}
				.tg .tg-491y{font-size:16px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;text-align:center;vertical-align:top}
				.tg .tg-ld9p{font-size:16px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:inherit;text-align:center;vertical-align:top}
				.tg .tg-1uwp{font-weight:bold;font-size:16px;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;text-align:center;vertical-align:top}
				</style>
				<table class="tg" style="undefined;table-layout: fixed; width: 932px;">
				<colgroup>
				<col style="width: 281px">
				<col style="width: 132px">
				<col style="width: 123px">
				<col style="width: 131px">
				<col style="width: 126px">
				<col style="width: 139px">
				</colgroup>
				  <tr>
				    <th class="tg-ld9p">mAP (%)<br># training images<br># testing images</th>
				    <th class="tg-491y">Location A (LA)<br>362<br>223</th>
				    <th class="tg-491y">Location B (LB)<br>355<br>230</th>
				    <th class="tg-491y">Location C (LC)<br>453<br>132</th>
				    <th class="tg-ld9p">Weather A (WA)<br>149<br>436</th>
				    <th class="tg-491y">Weather B (WB)<br>436<br>149</th>
				  </tr>
				  <tr>
				    <td class="tg-ld9p">ImageNet-AlexNet [1]</td>
				    <td class="tg-491y">15.00</td>
				    <td class="tg-491y">17.70</td>
				    <td class="tg-491y">14.14</td>
				    <td class="tg-ld9p">15.09</td>
				    <td class="tg-491y">16.15</td>
				  </tr>
				  <tr>
				    <td class="tg-491y">Places205-AlexNet [2]</td>
				    <td class="tg-491y">19.56</td>
				    <td class="tg-491y">19.33</td>
				    <td class="tg-491y">24.36</td>
				    <td class="tg-491y">17.96</td>
				    <td class="tg-491y">21.48</td>
				  </tr>
				  <tr>
				    <td class="tg-491y">IG-City8-Triplet [3]</td>
				    <td class="tg-491y">20.42</td>
				    <td class="tg-491y">20.43</td>
				    <td class="tg-491y">25.69</td>
				    <td class="tg-491y">18.56</td>
				    <td class="tg-491y">22.70</td>
				  </tr>
				  <tr>
				    <td class="tg-491y">Baseline (DT) [3]</td>
				    <td class="tg-491y">22.89</td>
				    <td class="tg-491y">24.07</td>
				    <td class="tg-491y">26.72</td>
				    <td class="tg-491y">32.19</td>
				    <td class="tg-491y">43.79</td>
				  </tr>
				  <tr>
				    <td class="tg-491y">CVDT (Max-pooling) [4]</td>
				    <td class="tg-491y">25.14</td>
				    <td class="tg-491y">24.97</td>
				    <td class="tg-491y">25.02</td>
				    <td class="tg-491y">33.04</td>
				    <td class="tg-491y">44.30</td>
				  </tr>
				  <tr>
				    <td class="tg-1uwp">CVDT (Cross-View Pooling)</td>
				    <td class="tg-1uwp">27.75</td>
				    <td class="tg-1uwp">26.81</td>
				    <td class="tg-1uwp">28.87</td>
				    <td class="tg-1uwp">34.47</td>
				    <td class="tg-1uwp">46.45</td>
				  </tr>
				  <tr>
				    <td class="tg-4saq">CVDT + Spatial (Da, Dx, Dy)</td>
				    <td class="tg-1uwp">35.00</td>
				    <td class="tg-1uwp">34.97</td>
				    <td class="tg-1uwp">35.70</td>
				    <td class="tg-4saq">42.60</td>
				    <td class="tg-1uwp">50.15</td>
				  </tr>
				</table>
				<p>
					<b>Cross-view visual learning.</b> mAP of different models on each subset. LA means that DT and CVDT are trained on Location B and Location C, and tested on Location A (i.e., buildings in Location A are unseen). WA and WB have seen all the buildings but trained and tested under different weather. \# means the number of images. The results indicates our proposed CVDT+CVP outperforms others on all subsets. (IG-City8 follows the same training process as [3] and forms triplet by IG-City8. CVDT (CVP)+Spatial is the method combines with Da+Dx+Dy.)
				</p>
		        	<img width=960 src="final_table.png"/>
		        	<p><b>Retrieval with relative spatial estimation.</b> The retrieval accuracy of line charts is drawn by different top-ranked images on five various subsets among different models. Each model is trained by other subsets. The value after line is mAP. The results show that our proposed method (CVDT+Da+Dx+Dy) can achieve the best performance under various settings.
		        	</p>
		        	<div id="exp">
		        		<div>
				        	<img width=480 src="supplemental/result_1.png"/>
				        	<img width=480 src="supplemental/result_2.png"/>
			        	</div>
			        	<hr/>
			        	<div>
				        	<img width=480 src="supplemental/result_3.png"/>
				        	<img width=480 src="supplemental/result_4.png"/>
			        	</div>
		        	</div>
				<li>Demo Video</li>
				<iframe width="960" height="540" src="https://www.youtube.com/embed/sdq31ep2zYk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
				<li>References</li>
				1. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012.<br/>
				2. Zhou, Bolei, et al. "Learning deep features for scene recognition using places database." Advances in neural information processing systems. 2014.<br/>
				3. Schroff, Florian, Dmitry Kalenichenko, and James Philbin. "Facenet: A unified embedding for face recognition and clustering." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.<br/>
				4. Su, Hang, et al. "Multi-view convolutional neural networks for 3d shape recognition." Proceedings of the IEEE international conference on computer vision. 2015. <br/>
				5. Ren, Shaoqing, et al. "Faster r-cnn: Towards real-time object detection with region proposal networks." Advances in neural information processing systems. 2015.
		        <li>
		        	Acknowledgement
		        </li>
		        This work was supported in part by MediaTek Inc and the Ministry of Science and Technology, Taiwan, under Grant MOST 107-2634-F-002-007. We also benefit from the grants from NVIDIA and the NVIDIA DGX-1 AI Supercomputer.
		        <li>
		        	Contact
		        </li>
		        If you have any problems about our work, do not hesitate to contact me <a href="mailto: jacky82226@gmail.com">jacky82226@gmail.com</a>.
    		</ul>
    		</div>
    	</div>
	</body>
</html>